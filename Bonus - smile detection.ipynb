{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: on-line smile detection\n",
    "\n",
    "In this lab, you will write a classifier that recognises automatically whether the people behind behind the computer are smiling or not. This is a challenging classification problem, and requires us to deal with: \n",
    "1. handling the camera hardware\n",
    "1. Detecting the face(s) in the image\n",
    "1. Deciding whether the face contains a smile.\n",
    "\n",
    "To help you implement this, the following code reads in the images from the webcam, detects any faces, resizes the detected faces to a cannonical size of 40 by 40 pixels and calls the classification function (which you need to implement) to decide whether that face sports a smile or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import the libraries that we need\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "face_target_size = (48,48)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smile detection\n",
    "\n",
    "In the lecture, we have seen different types of classifiers. We have looked at naive probabilistic classifiers, which consider each dimension independently, and we have seen more advanced classifiers (such as linear classifiers and Decision Trees) which consider the relations between the different dimensions in the data. For the bonus, you can use existing implementations of classifiers and the challenge is to figure out what kind of classifier is appropriate for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hasSmile(face):\n",
    "    # Your implementation comes here\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "You are given the following labelled data to train your classifier on. It consists of 7519 images of smiles, and 5221 images of other faces. The following code reads them into two matrices, where each row contains the pixels of one image; it is up to you to convert them to a format that your classifier understands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7519 5221\n",
      "(7519, 2304) (5221, 2304)\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "posdir = 'smiles/pos'\n",
    "negdir = 'smiles/neg'\n",
    "\n",
    "pos = listdir(posdir)\n",
    "neg = listdir(negdir)\n",
    "print(len(pos),len(neg))\n",
    "\n",
    "posfaces = np.zeros((len(pos),face_target_size[0]*face_target_size[1]))\n",
    "\n",
    "Visualise = False\n",
    "if Visualise:\n",
    "    print(\"Press 'q' to stop the iteration!\")\n",
    "    print(\"Set 'Visualise=False' to speed things up.\")\n",
    "\n",
    "i=0\n",
    "for p in pos:\n",
    "    im = cv2.imread(posdir+'/'+p, cv2.IMREAD_GRAYSCALE)\n",
    "    if Visualise:\n",
    "        print(p, im.shape)\n",
    "        cv2.imshow(\"face\",im)\n",
    "    posfaces[i,:] = im.flatten()\n",
    "    i += 1\n",
    "    \n",
    "    if Visualise:\n",
    "        k = cv2.waitKey(500) \n",
    "        if k!=-1:\n",
    "            print(k)\n",
    "        if k & 0xFF == ord('q'):\n",
    "            break\n",
    "if Visualise:    \n",
    "    cv2.destroyWindow(\"face\")\n",
    "\n",
    "negfaces = np.zeros((len(neg),face_target_size[0]*face_target_size[1]))\n",
    "\n",
    "i=0\n",
    "for p in neg:\n",
    "    im = cv2.imread(negdir+'/'+p, cv2.IMREAD_GRAYSCALE)\n",
    "    negfaces[i,:] = im.flatten()\n",
    "    i+=1\n",
    "\n",
    "print(posfaces.shape,negfaces.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To evaluate your implementation, we will run it on a separate test set of face images. This set is safely kept in secret conditions and you are not allowed to see it or use it to improve your classifier. This is to avoid overfitting: you cannot tweak your classifier to perform well on the particular data that we will evaluate you on. \n",
    "\n",
    "But you should be able to evaluate your work as well, before you submit it. This requires you to have labelled data too. You should therefore split the data you have loaded in, into a train and a validation set. The performance on the train set indicates how well your classifier is learning from that data; the performance on the validation set gives you an indication of how well the classifier will perform on future data.\n",
    "\n",
    "## Visualisation\n",
    "\n",
    "You can additionally evaluate your method on yourself with the following code. This code uses the first webcam it detects and shows the captured image. The detected faces are shown with a box, and this box becomes red if your code detects a smile. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "from utils.inference import detect_faces\n",
    "from utils.inference import draw_text\n",
    "from utils.inference import draw_bounding_box\n",
    "from utils.inference import apply_offsets\n",
    "from utils.inference import load_detection_model\n",
    "\n",
    "face_detection = load_detection_model(\"./haarcascade_frontalface_default.xml\")\n",
    "\n",
    "emotion_offsets = (20, 40)\n",
    "\n",
    "cv2.namedWindow('webcam')\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret,bgr_image = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "    faces = detect_faces(face_detection, gray_image)        \n",
    "\n",
    "    for face_coordinates in faces:\n",
    "        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "        if x2<=x1 or y2 <= y1:\n",
    "            continue\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "        gray_face = cv2.resize(gray_face, face_target_size)\n",
    "\n",
    "        color = (255,255,255)\n",
    "        if hasSmile(gray_face): \n",
    "            print(x1,x2,y1,y2)\n",
    "            color = (255,0,0)\n",
    "        x,y,w,h = face_coordinates\n",
    "\n",
    "        draw_bounding_box(face_coordinates, rgb_image, color)\n",
    "#        cv2.rectangle(rgb_image, (x,y-5),(x+w,y-15), color, 1)\n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cv2.imshow('webcam',bgr_image)\n",
    "    k = cv2.waitKey(10) \n",
    "    if k!=-1:\n",
    "        print(k)\n",
    "    if k & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyWindow(\"webcam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
